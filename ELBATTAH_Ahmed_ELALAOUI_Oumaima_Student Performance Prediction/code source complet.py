# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O8anAONxcKbc2GH5fXWRanTh8KmEKrUr

Charger le ficher csv
"""

import pandas as pd

# Remplace 'nom_du_fichier.csv' par le nom r√©el de ton fichier
df = pd.read_csv('/content/drive/MyDrive/StudentPerformanceFactors.csv')

# Afficher les 5 premi√®res lignes
print(df.head())

"""Exploration des donn√©es

1Ô∏è‚É£ V√©rifier la structure du dataset
"""

# Informations g√©n√©rales sur les colonnes et leur type
print(df.info())

# Statistiques g√©n√©rales des colonnes num√©riques
print(df.describe())

"""2Ô∏è‚É£ V√©rifier les valeurs manquantes"""

print(df.isnull().sum())

"""3Ô∏è‚É£ V√©rifier les valeurs uniques des variables cat√©gorielles"""

# Liste des valeurs uniques dans chaque colonne cat√©gorielle
for col in df.select_dtypes(include=['object']).columns:
    print(f"{col}: {df[col].unique()}")

"""# Clean"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# 1Ô∏è‚É£ Charger le dataset depuis Google Drive
file_path = "/content/drive/MyDrive/StudentPerformanceFactors.csv"  # Chemin correct
df = pd.read_csv(file_path)

# 2Ô∏è‚É£ Supprimer les doublons
df.drop_duplicates(inplace=True)

# 3Ô∏è‚É£ Remplir les valeurs manquantes
# Pour les colonnes num√©riques, on remplit avec la moyenne
df = df.fillna(df.mean(numeric_only=True))

# Pour les colonnes cat√©gorielles, on remplit avec la valeur la plus fr√©quente (mode)
for col in df.select_dtypes(include=['object']).columns:
    df[col] = df[col].fillna(df[col].mode()[0])

# 4Ô∏è‚É£ Encoder les variables cat√©gorielles
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])

# 5Ô∏è‚É£ G√©rer les valeurs extr√™mes (outliers)
def handle_outliers(df):
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
    for col in numeric_cols:
        # Calculer le premier et troisi√®me quartile, ainsi que l'IQR (Interquartile Range)
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1

        # Calcul des bornes pour identifier les outliers
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Remplacer les outliers par les bornes (ou utiliser la m√©diane, √† ajuster selon votre pr√©f√©rence)
        if col == 'Tutoring_Sessions':  # Traitement sp√©cifique pour la colonne 'Tutoring_Sessions'
            median_value = df[col].median()  # Calcul de la m√©diane
            df[col] = df[col].apply(lambda x: median_value if x < lower_bound or x > upper_bound else x)
        else:
            df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)

        # Identifier et afficher les outliers
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        print(f"üîç Valeurs extr√™mes dans la colonne '{col}' : {len(outliers)} outliers d√©tect√©s.")
    return df

df = handle_outliers(df)

# 6Ô∏è‚É£ Conversion explicite des colonnes num√©riques en 'int' ou 'float'
for col in df.select_dtypes(include=['int64', 'float64']).columns:
    try:
        # Si la colonne est float64, on peut l'arrondir et la convertir en int64
        if df[col].dtype == 'float64':
            df[col] = df[col].round().astype('int64')  # Arrondir et convertir en int64
        else:
            df[col] = pd.to_numeric(df[col], errors='raise')  # Conversion explicite
    except ValueError:
        print(f"Erreur lors de la conversion de la colonne {col} en num√©rique.")

# 7Ô∏è‚É£ Sauvegarder le dataset nettoy√©
cleaned_file_path = "/content/drive/MyDrive/Cleaned_student-performance.csv"
df.to_csv(cleaned_file_path, index=False)
print(f"‚úÖ Dataset nettoy√© enregistr√© sous : {cleaned_file_path}")

from google.colab import files
files.download('/content/drive/MyDrive/Cleaned_student-performance.csv')

"""*Verification*"""

# V√©rification des doublons, des valeurs manquantes, des types de donn√©es et des outliers

# V√©rification des doublons
def check_duplicates(df):
    duplicates = df[df.duplicated()]
    if not duplicates.empty:
        print(f"üî¥ Il y a {len(duplicates)} doublons dans le dataset.")
    else:
        print("üü¢ Aucun doublon trouv√© dans le dataset.")

check_duplicates(df)

# V√©rification des valeurs manquantes
def check_missing_values(df):
    missing = df.isnull().sum()
    if missing.any():
        print("üî¥ Valeurs manquantes d√©tect√©es :")
        print(missing[missing > 0])
    else:
        print("üü¢ Aucune valeur manquante d√©tect√©e dans le dataset.")

check_missing_values(df)

# V√©rification des types de donn√©es
def check_data_types(df):
    print("üìù Types de donn√©es des colonnes :")
    print(df.dtypes)
    # V√©rifier que toutes les colonnes sont du type 'int' ou 'float' (selon tes besoins)
    if df.dtypes.value_counts().get('int64', 0) == df.shape[1]:
        print("üü¢ Toutes les colonnes sont de type 'int64'.")
    else:
        print("üî¥ Attention, certaines colonnes ne sont pas de type 'int64'.")

check_data_types(df)

# V√©rification des valeurs extr√™mes (outliers) dans le dataset
def check_outliers(df):
    print("üìä V√©rification des valeurs extr√™mes (outliers) :")
    numeric_cols = df.select_dtypes(include=['int64']).columns
    for col in numeric_cols:
        q1 = df[col].quantile(0.25)
        q3 = df[col].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        print(f"üîç Valeurs extr√™mes dans la colonne '{col}' : {len(outliers)} outliers d√©tect√©s.")

check_outliers(df)

"""1. Analyse exploratoire des donn√©es (EDA) :

affichage plus stati
"""

# Afficher les premi√®res lignes du dataset
df.head()
# Statistiques descriptives pour les donn√©es num√©riques
df.describe()

"""V√©rification de la corr√©lation"""

import matplotlib.pyplot as plt
import seaborn as sns

# Matrice de corr√©lation
corr_matrix = df.corr()

# Afficher la heatmap de la corr√©lation
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title("Matrice de corr√©lation")
plt.show()

"""1.4 Visualisation de la distribution des variables num√©riques"""

# Histogrammes pour visualiser la distribution des variables num√©riques
df.hist(figsize=(12, 10), bins=20)
plt.tight_layout()
plt.show()

# Boxplots pour d√©tecter les outliers
for col in df.select_dtypes(include=['int64', 'float64']).columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=df[col])
    plt.title(f"Boxplot de {col}")
    plt.show()

"""1.5 V√©rification des donn√©es manquantes (si applicable)

"""

# V√©rification des valeurs manquantes
missing_values = df.isnull().sum()
print(f"Valeurs manquantes par colonne:\n{missing_values}")

"""√âtape 2 : Pr√©paration des donn√©es pour le mod√®le

> Add blockquote

2.1 S√©lection des variables et de la cible
"""

# S√©paration des variables ind√©pendantes (X) et de la cible (y)
X = df.drop(columns=['Exam_Score'])  # Variables ind√©pendantes
y = df['Exam_Score']  # Variable cible (performance des √©tudiants)

"""2.2 Diviser les donn√©es en ensembles d'entra√Ænement et de test



"""

from sklearn.model_selection import train_test_split

# Diviser les donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Afficher la taille des ensembles pour v√©rifier
print(f"Taille de l'ensemble d'entra√Ænement : {X_train.shape}")
print(f"Taille de l'ensemble de test : {X_test.shape}")

"""2.3 Normalisation des donn√©es (facultatif mais recommand√©)"""

from sklearn.preprocessing import StandardScaler

# Normaliser les donn√©es (entrainement et test)
scaler = StandardScaler()

# Ajuster le scaler sur les donn√©es d'entra√Ænement et transformer les donn√©es d'entra√Ænement
X_train_scaled = scaler.fit_transform(X_train)

# Transformer les donn√©es de test en utilisant le m√™me scaler
X_test_scaled = scaler.transform(X_test)

# V√©rification de la normalisation
print(f"Exemple de donn√©es normalis√©es (X_train_scaled) : \n{X_train_scaled[:5]}")

"""√âtape 1 : Appliquer l'algorithme K-means



"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples

# ========= Pr√©paration des donn√©es =========
# On suppose que X_train_scaled (donn√©es normalis√©es) et y_train (les cibles) sont d√©j√† d√©finis.
# Exemple si besoin :
# scaler = StandardScaler()
# X_train_scaled = scaler.fit_transform(X_train)

# ========= R√©duction de dimension avec PCA =========
# On r√©duit √† 2 dimensions pour faciliter le clustering et la visualisation
pca = PCA(n_components=2, random_state=42)
X_train_pca = pca.fit_transform(X_train_scaled)

# ========= Recherche du meilleur nombre de clusters =========
silhouette_scores = []
k_values = range(2, 7)
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=50)
    kmeans.fit(X_train_pca)
    score = silhouette_score(X_train_pca, kmeans.labels_)
    silhouette_scores.append(score)
    print(f"k = {k} -> Score de silhouette = {score:.2f}")

plt.figure(figsize=(8,6))
plt.plot(list(k_values), silhouette_scores, marker='o', linestyle='-', color='blue')
plt.title("Score de Silhouette pour diff√©rents nombres de clusters")
plt.xlabel("Nombre de clusters")
plt.ylabel("Score de Silhouette")
plt.grid(True)
plt.show()

best_k = list(k_values)[np.argmax(silhouette_scores)]
print(f"Le meilleur nombre de clusters est : {best_k}")

# ========= Application finale de K-means avec le meilleur k =========
kmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=50)
kmeans_final.fit(X_train_pca)
silhouette_final = silhouette_score(X_train_pca, kmeans_final.labels_)
print(f"Score de silhouette pour {best_k} clusters : {silhouette_final:.2f}")

# ========= Visualisation des clusters =========
plt.figure(figsize=(8,6))
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1],
            c=kmeans_final.labels_, cmap='viridis', alpha=0.7)
plt.title(f"Clustering K-means (k={best_k}) apr√®s PCA")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.colorbar(label='Cluster')
plt.show()

# ========= Visualisation des centroids =========
centers_final = kmeans_final.cluster_centers_
plt.figure(figsize=(8,6))
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1],
            c=kmeans_final.labels_, cmap='viridis', alpha=0.6)
plt.scatter(centers_final[:, 0], centers_final[:, 1],
            c='red', marker='X', s=200, label='Centroids')
plt.title(f"Centroids des clusters (k={best_k})")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend()
plt.show()


# Ajout des labels finaux au DataFrame pour une analyse plus pouss√©e
df_clustered_final = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2'])
df_clustered_final['Cluster'] = kmeans_final.labels_
df_clustered_final['Exam_Score'] = y_train.values
print("Moyennes des caract√©ristiques pour chaque cluster (best_k) :")
print(df_clustered_final.groupby('Cluster').mean())

"""√âtape 2 : Appliquer l'algorithme APRIORI"""

print(df.head())  # Aper√ßu des 5 premi√®res lignes
print(df.info())  # Informations sur les types de donn√©es
print(df.describe())  # Statistiques g√©n√©rales
print(df.isnull().sum())  # V√©rification des valeurs manquantes

"""‚úÖ Analyse de tes donn√©es pour Apriori

üî• Premi√®re t√¢che pour toi : Transformer tes donn√©es

---
"""

import pandas as pd

# Charger tes donn√©es
df = pd.read_csv("/content/drive/MyDrive/Cleaned_student-performance.csv")  # Si tu as un fichier CSV

# Transformation des variables continues en cat√©gories
df['√âtudie beaucoup'] = df['Hours_Studied'].apply(lambda x: 1 if x > 20 else 0)
df['Pr√©sence √©lev√©e'] = df['Attendance'].apply(lambda x: 1 if x > 80 else 0)
df['Bon √©l√®ve'] = df['Previous_Scores'].apply(lambda x: 1 if x > 70 else 0)

# Encodage des variables cat√©goriques
df = pd.get_dummies(df, columns=['Motivation_Level', 'Parental_Involvement'])

# Suppression des colonnes inutiles
df = df.drop(columns=['Hours_Studied', 'Attendance', 'Previous_Scores'])

# Afficher l'aper√ßu des nouvelles donn√©es
print(df.head())

from mlxtend.frequent_patterns import apriori, association_rules
import networkx as nx
import matplotlib.pyplot as plt

# --- Pr√©paration pour Apriori ---
# Convertir toutes les colonnes bool√©ennes en entiers (0 et 1)
for col in df.columns:
    if df[col].dtype == 'bool':
        df[col] = df[col].astype(int)

# Liste des colonnes binaires pertinentes pour Apriori
binary_cols = [
    '√âtudie beaucoup', 'Pr√©sence √©lev√©e', 'Bon √©l√®ve',
    'Motivation_Level_0', 'Motivation_Level_1', 'Motivation_Level_2',
    'Parental_Involvement_0', 'Parental_Involvement_1', 'Parental_Involvement_2'
]

# Cr√©er le DataFrame pour Apriori
df_apriori = df[binary_cols]

# Convertir le DataFrame en bool√©en pour am√©liorer la performance et √©viter l'avertissement
df_apriori = df_apriori.astype(bool)

# --- Application de l'algorithme Apriori ---
# G√©n√©rer les ensembles fr√©quents avec un support minimum
frequent_itemsets = apriori(df_apriori, min_support=0.05, use_colnames=True)

# Extraire les r√®gles d'association en utilisant le lift comme m√©trique
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)

# Afficher les 10 r√®gles d'association avec le plus de lift, tri√©es par ordre d√©croissant
print("Les 10 r√®gles d'association avec le plus de lift:")
print(rules.sort_values(by='lift', ascending=False).head(10))

# --- Visualisation des r√®gles d'association sous forme de graphe ---
# Cr√©er un graphe orient√© pour repr√©senter les r√®gles
G = nx.DiGraph()
for idx, row in rules.iterrows():
    for antecedent in row['antecedents']:
        for consequent in row['consequents']:
            G.add_edge(antecedent, consequent, weight=row['lift'])

# Dessiner le graphe
plt.figure(figsize=(10, 6))
pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray',
        node_size=2000, font_size=10)
plt.title("Graphique des r√®gles d'association")
plt.show()

# --- Exportation des r√®gles d'association ---
rules.to_csv("regles_association.csv", index=False)
print("Les r√®gles d'association ont √©t√© export√©es dans 'regles_association.csv'")

"""√âtape 3 : Appliquer l'algorithme KNN

1. Cr√©ation d'une variable cible cat√©gorielle

1.   List item
2.   List item
"""

import pandas as pd

# Charger le dataset nettoy√©
df = pd.read_csv("/content/drive/MyDrive/Cleaned_student-performance.csv")

# Cr√©er une nouvelle colonne "Score_Category"
df['Score_Category'] = df['Exam_Score'].apply(lambda x: 'High' if x >= 67 else 'Low')

# Afficher un aper√ßu
print(df[['Exam_Score', 'Score_Category']].head())

"""2. Pr√©paration des donn√©es pour K-NN"""

from sklearn.model_selection import train_test_split

# On choisit certaines colonnes pertinentes pour la pr√©diction.
# Tu peux ajuster cette s√©lection selon ton analyse.
features = df.drop(columns=['Exam_Score', 'Score_Category'])
target = df['Score_Category']

# Diviser en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)

print(f"Taille de l'ensemble d'entra√Ænement : {X_train.shape}")
print(f"Taille de l'ensemble de test : {X_test.shape}")

"""3. Normalisation des donn√©es

"""

from sklearn.preprocessing import StandardScaler

scaler_knn = StandardScaler()
X_train_scaled = scaler_knn.fit_transform(X_train)
X_test_scaled = scaler_knn.transform(X_test)

"""4. Application de K-NN et √©valuation"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Cr√©er le mod√®le K-NN avec un nombre de voisins initial (par exemple k=5)
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

# Pr√©dire sur l'ensemble de test
y_pred = knn.predict(X_test_scaled)

# √âvaluer le mod√®le
accuracy = accuracy_score(y_test, y_pred)
print(f"Exactitude du mod√®le K-NN : {accuracy:.2f}")

print("Matrice de confusion:")
print(confusion_matrix(y_test, y_pred))

print("Rapport de classification:")
print(classification_report(y_test, y_pred))

"""5. Optimisation du param√®tre k (nombre de voisins)





"""

import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score

k_values = range(1, 21)
cv_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    # Utilisation de la validation crois√©e √† 5 plis sur l'ensemble d'entra√Ænement
    scores = cross_val_score(knn, X_train_scaled, y_train, cv=5, scoring='accuracy')
    cv_scores.append(scores.mean())

# Afficher les scores de validation pour chaque k
plt.figure(figsize=(8,6))
plt.plot(k_values, cv_scores, marker='o', linestyle='-', color='blue')
plt.xlabel('Nombre de voisins (k)')
plt.ylabel('Score moyen de validation crois√©e')
plt.title('Validation crois√©e pour K-NN')
plt.grid(True)
plt.show()

best_k = k_values[cv_scores.index(max(cv_scores))]
print(f"Le meilleur nombre de voisins (k) est : {best_k}")

"""r√©entra√Æner ton mod√®le avec ce meilleur k"""

# R√©entra√Æner le mod√®le K-NN avec le meilleur k
knn_best = KNeighborsClassifier(n_neighbors=best_k)
knn_best.fit(X_train_scaled, y_train)

# √âvaluer sur l'ensemble de test
y_pred_best = knn_best.predict(X_test_scaled)
best_accuracy = accuracy_score(y_test, y_pred_best)
print(f"Exactitude du mod√®le K-NN optimis√© (k={best_k}) : {best_accuracy:.2f}")

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Calcul de la matrice de confusion
cm = confusion_matrix(y_test, y_pred)

# Visualisation de la matrice de confusion
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Bas', 'Moyenne', 'Haut'], yticklabels=['Bas', 'Moyenne', 'Haut'])
plt.title('Matrice de Confusion - K-NN')
plt.xlabel('Pr√©diction')
plt.ylabel('R√©alit√©')
plt.show()

print("Rapport de classification:")
print(classification_report(y_test, y_pred))